{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0f329e9",
   "metadata": {},
   "source": [
    "Using openAI to clean the text with proper context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972dea7",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "# Load environment variables from a .env file.\n",
    "# Make sure you have a .env file in the same directory with the line:\n",
    "# OPENAI_API_KEY=\"your_key_here\"\n",
    "load_dotenv()\n",
    "\n",
    "# Set your OpenAI API key from the environment variable.\n",
    "# The script will use the openai.OpenAI() client which automatically\n",
    "# looks for this environment variable.\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file or environment variables. Please add it.\")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def read_instruction_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads the content of an instruction file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the instruction file.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the file, or an empty string if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Instruction file not found at {file_path}. An empty string will be used for the system prompt.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_text_batch_json_openai(batch_dict, model_name=\"gpt-4o-latest\", system_prompt_content=\"\", temperature=1):\n",
    "    \"\"\"\n",
    "    Cleans a batch of texts using a single OpenAI API call with JSON mode.\n",
    "\n",
    "    Args:\n",
    "        batch_dict (dict): A dictionary of texts to clean, with the original index as the key.\n",
    "                           Example: {10: \"text to clean\", 15: \"another text\"}\n",
    "        model_name (str): The name of the OpenAI model to use.\n",
    "        system_prompt_content (str): The system prompt content to use for cleaning.\n",
    "        temperature (float): The sampling temperature for generation (0.0 to 2.0).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with original indices as keys and cleaned texts as values.\n",
    "              Returns an empty dictionary on failure.\n",
    "    \"\"\"\n",
    "    # --- Read instructions from file to use as the system prompt ---\n",
    "    # This guides the model's behavior for the entire conversation.\n",
    "    # Use the system_prompt_content passed as parameter, or read from file if empty\n",
    "    if not system_prompt_content:\n",
    "        system_prompt_content = read_instruction_file('system_prompt_cleaning_v2.txt')\n",
    "\n",
    "    # --- Construct the user prompt ---\n",
    "    # The user prompt provides the data and instructs the model on the desired output format.\n",
    "    # We serialize the dictionary of texts into a JSON string to send to the model.\n",
    "    texts_to_clean_json = json.dumps(batch_dict, indent=2)\n",
    "    user_prompt = f\"\"\"\n",
    "Please clean each text in the following JSON object based on the rules provided in the system prompt.\n",
    "The keys in the object are the original identifiers.\n",
    "Your response must be a single, valid JSON object that contains the exact same keys as the input,\n",
    "but with the cleaned text as the corresponding values.\n",
    "Do not add any commentary, explanations, or markdown formatting around your response.\n",
    "Your entire output must be only the JSON object.\n",
    "\n",
    "Input Texts:\n",
    "{texts_to_clean_json}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # --- Call the OpenAI API ---\n",
    "        # We use the Chat Completions endpoint, which is standard for GPT-4 models.\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            # We enable JSON Mode to guarantee the output is a valid JSON string.\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=temperature,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt_content},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # print(\"user prompt:\", user_prompt)\n",
    "        # The model's response content should be a JSON string.\n",
    "        # We parse it into a Python dictionary.\n",
    "        response_content = response.choices[0].message.content\n",
    "        cleaned_results_dict = json.loads(response_content)\n",
    "        return cleaned_results_dict\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Failed to decode JSON from model response. Error: {e}\")\n",
    "        # It's helpful to see what the model returned if it wasn't valid JSON.\n",
    "        if 'response_content' in locals():\n",
    "            print(f\"Model response was:\\n---\\n{response_content}\\n---\")\n",
    "        return {}\n",
    "    except openai.APIError as e:\n",
    "        print(f\"An OpenAI API error occurred: {e}\")\n",
    "        time.sleep(1) # Wait a moment before potential retries\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during the API call: {e}\")\n",
    "        time.sleep(1)\n",
    "        return {}\n",
    "\n",
    "\n",
    "def process_csv_openai(input_file, output_file, model_name, system_prompt_content, batch_size=50):\n",
    "    \"\"\"\n",
    "    Reads a CSV, processes a column in batches using the OpenAI API, and saves the result.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "        output_file (str): Path to the output CSV file.\n",
    "        model_name (str): The name of the OpenAI model to use.\n",
    "        system_prompt_content (str): The system prompt content to use for cleaning.\n",
    "        batch_size (int): Number of rows to process in each batch.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at {input_file}\")\n",
    "        return\n",
    "\n",
    "    # Ensure the required column exists\n",
    "    if 'Segment Text' not in df.columns:\n",
    "        print(\"Error: 'Segment Text' column not found in the CSV file.\")\n",
    "        return\n",
    "\n",
    "    # Create a new column for cleaned text, initialized with a placeholder.\n",
    "    df['Cleaned Text'] = pd.NA\n",
    "\n",
    "    # Get a series of the text to be processed, dropping any empty values.\n",
    "    texts_to_process = df['Segment Text'].dropna()\n",
    "    \n",
    "    # Process texts in batches to manage API request size.\n",
    "    num_batches = (len(texts_to_process) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(texts_to_process), batch_size):\n",
    "        batch_series = texts_to_process.iloc[i:i + batch_size]\n",
    "        \n",
    "        # Convert the batch Series to a dictionary.\n",
    "        # The dictionary's keys are the original DataFrame indices, which is crucial\n",
    "        # for mapping the results back to the correct rows.\n",
    "        batch_dict = batch_series.to_dict()\n",
    "        \n",
    "        print(f\"Processing batch {i // batch_size + 1}/{num_batches} (Rows {min(batch_dict.keys())} to {max(batch_dict.keys())})...\")\n",
    "\n",
    "        # Use the model_name and system_prompt_content passed as parameters\n",
    "        cleaned_batch_dict = clean_text_batch_json_openai(batch_dict, model_name, system_prompt_content)\n",
    "            \n",
    "        if cleaned_batch_dict:\n",
    "            # Map the cleaned results back to the 'Cleaned Text' column.\n",
    "            # We iterate through the returned dictionary and use the keys (original indices)\n",
    "            # to place the cleaned text in the correct location in the DataFrame.\n",
    "            for original_index, cleaned_text in cleaned_batch_dict.items():\n",
    "                # The keys from the JSON response might be strings, so convert them to int.\n",
    "                df.loc[int(original_index), 'Cleaned Text'] = cleaned_text\n",
    "        else:\n",
    "            print(f\"Warning: Batch {i // batch_size + 1} returned no data.\")\n",
    "\n",
    "\n",
    "    # Optionally, drop the original column after processing\n",
    "    df = df.drop(columns=['Segment Text'])\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file.\n",
    "    # Using 'utf-8-sig' helps Excel open the file correctly with special characters.\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nProcessing complete. Cleaned data saved to '{output_file}'\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- DEFINE YOUR FILE PATHS HERE ---\n",
    "    # IMPORTANT: Replace this with the actual path to your input CSV file.\n",
    "    # Using a raw string (r'...') is helpful on Windows to avoid issues with backslashes.\n",
    "    input_csv_path =  r'D:\\SOKM\\11 Identity 2 SoKM 2024 - 2025\\11 Identity 2 SoKM 2024 - 2025_transcript_english_SE_br_converted.csv'\n",
    "\n",
    "    # Define the model name you want to use\n",
    "    model_name = \"gpt-4.1\"  # Choose the model you want to use\n",
    "    \n",
    "    # --- Read the system prompt from a file ---\n",
    "    system_prompt_content = read_instruction_file('system_prompt_cleaning_v2.3.txt')\n",
    "\n",
    "    # Use regex to remove any special characters from the model name to create an output file name\n",
    "    model_name_cleaned = re.sub(r'[^a-zA-Z0-9]', '_', model_name)\n",
    "    \n",
    "    # Define the output file path automatically\n",
    "    output_csv_path = input_csv_path.replace('.csv', f'_cleaned_{model_name_cleaned}.csv')\n",
    "\n",
    "    # This batch size determines how many rows are sent in a single API request.\n",
    "    # A size of 25-100 is a good starting point, but you can adjust it based on\n",
    "    # the average length of your text to avoid exceeding model token limits.\n",
    "    process_csv_openai(input_csv_path, output_csv_path, model_name, system_prompt_content, batch_size=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1351f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance of the cleaning process\n",
    "# This section is for evaluating the performance of the cleaning process.\n",
    "# You can use the WER (Word Error Rate) to measure how well the cleaning process performed.\n",
    "\n",
    "import jiwer\n",
    "# Example of how to use jiwer to calculate WER\n",
    "def calculate_wer(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculate the Word Error Rate (WER) between a reference and a hypothesis.\n",
    "\n",
    "    Args:\n",
    "        reference (str): The ground truth text.\n",
    "        hypothesis (str): The cleaned text to compare against the reference.\n",
    "\n",
    "    Returns:\n",
    "        float: The WER value.\n",
    "    \"\"\"\n",
    "    return jiwer.wer(reference, hypothesis)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the cleaned data and original data\n",
    "    cleaned_df = pd.read_csv(output_csv_path)\n",
    "    original_df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Ensure cleaned_df has the necessary columns\n",
    "    # Add the original Segment Text back to the cleaned DataFrame\n",
    "    if 'Segment Text' not in cleaned_df.columns:\n",
    "        cleaned_df['Segment Text'] = original_df['Segment Text']\n",
    "    \n",
    "    # Verify both required columns exist\n",
    "    if 'Cleaned Text' not in cleaned_df.columns:\n",
    "        print(\"Error: 'Cleaned Text' column not found in cleaned data.\")\n",
    "    else:\n",
    "        # Calculate WER for each row by comparing original and cleaned text\n",
    "        wer_scores = []\n",
    "        for idx in range(len(cleaned_df)):\n",
    "            if pd.notna(cleaned_df.loc[idx, 'Segment Text']) and pd.notna(cleaned_df.loc[idx, 'Cleaned Text']):\n",
    "                wer_score = calculate_wer(\n",
    "                    str(cleaned_df.loc[idx, 'Segment Text']), \n",
    "                    str(cleaned_df.loc[idx, 'Cleaned Text'])\n",
    "                )\n",
    "                wer_scores.append(wer_score)\n",
    "            else:\n",
    "                wer_scores.append(None)  # Handle missing data\n",
    "        \n",
    "        cleaned_df['WER'] = wer_scores\n",
    "        \n",
    "        # Reorder columns to have Segment Text, Cleaned Text, and WER\n",
    "        column_order = ['Segment Text', 'Cleaned Text', 'WER']\n",
    "        # Add any other columns that might exist\n",
    "        other_columns = [col for col in cleaned_df.columns if col not in column_order]\n",
    "        final_columns = column_order + other_columns\n",
    "        cleaned_df = cleaned_df[final_columns]\n",
    "        \n",
    "        # Save the results with WER to a new CSV file\n",
    "        wer_output_path = output_csv_path.replace('.csv', '_with_wer.csv')\n",
    "        cleaned_df.to_csv(wer_output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nWER results saved to '{wer_output_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b6ca1",
   "metadata": {},
   "source": [
    "Using Gemini to clean the text with proper context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd927d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your Google API key\n",
    "# Ensure you have a .env file with GOOGLE_API_KEY=\"your_key_here\"\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in .env file. Please add it.\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "\n",
    "def read_instruction_file(file_path):\n",
    "    \"\"\"Reads the content of an instruction file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Instruction file not found at {file_path}. An empty string will be used.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_text_batch_json(batch_dict, system_prompt_content, model_name=\"gemini-2.5-flash-latest\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Cleans a batch of texts using a single API call with JSON mode.\n",
    "\n",
    "    Args:\n",
    "        batch_dict (dict): A dictionary of texts to clean, with original index as key.\n",
    "                           Example: {10: \"text to clean\", 15: \"another text\"}\n",
    "        system_prompt_content (str): The system prompt content to use for cleaning.\n",
    "        model_name (str): The name of the Gemini model to use.\n",
    "        temperature (float): The temperature for generation.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with original indices as keys and cleaned texts as values.\n",
    "              Returns an empty dictionary on failure.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Construct the user prompt ---\n",
    "    # The user prompt contains the data and the specific instructions for formatting the output.\n",
    "    # We serialize the dictionary of texts into a JSON string to send to the model.\n",
    "    texts_to_clean_json = json.dumps(batch_dict, indent=2)\n",
    "    user_prompt = f\"\"\"\n",
    "Please clean each text in the following JSON object.\n",
    "The keys are the original identifiers.\n",
    "Return a single, valid JSON object that contains the exact same keys, but with the cleaned text as the values.\n",
    "Do not add any commentary, explanations, or markdown formatting around your response. Your entire output must be only the JSON object.\n",
    "\n",
    "Input Texts:\n",
    "{texts_to_clean_json}\n",
    "\"\"\"\n",
    "\n",
    "    # --- Set up the Gemini model for JSON output ---\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=model_name,\n",
    "        system_instruction=system_prompt_content,\n",
    "        generation_config={\n",
    "            \"temperature\": temperature,\n",
    "            # Crucially, we tell the model we expect a JSON response.\n",
    "            \"response_mime_type\": \"application/json\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Call the Gemini API to generate the cleaned content\n",
    "        response = model.generate_content(user_prompt)\n",
    "        \n",
    "        # The model's response should be a JSON string. We parse it into a Python dictionary.\n",
    "        cleaned_results_dict = json.loads(response.text)\n",
    "        return cleaned_results_dict\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Failed to decode JSON from model response. Error: {e}\")\n",
    "        print(f\"Model response was:\\n---\\n{response.text}\\n---\")\n",
    "        return {} # Return empty dict on failure\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during API call: {e}\")\n",
    "        # Optional: Add a small delay to handle potential rate limiting issues\n",
    "        time.sleep(1)\n",
    "        return {} # Return empty dict on failure\n",
    "\n",
    "\n",
    "def process_csv(input_file, output_file, model_name, system_prompt_content, batch_size=50):\n",
    "    \"\"\"\n",
    "    Reads a CSV, processes a specific column in batches using the Gemini API's JSON mode,\n",
    "    and saves the result.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "        output_file (str): Path to the output CSV file.\n",
    "        model_name (str): The name of the Gemini model to use.\n",
    "        system_prompt_content (str): The system prompt content to use for cleaning.\n",
    "        batch_size (int): Number of rows to process in each batch.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at {input_file}\")\n",
    "        return\n",
    "\n",
    "    if 'Segment Text' not in df.columns:\n",
    "        print(\"Error: The 'Segment Text' column is not present in the CSV file.\")\n",
    "        return\n",
    "\n",
    "    # Create a new column for the cleaned text, initialized as empty\n",
    "    df['Cleaned Text'] = pd.NA\n",
    "\n",
    "    # Get a series of the text to be processed, dropping any empty rows\n",
    "    texts_to_process = df['Segment Text'].dropna()\n",
    "    \n",
    "    # Process texts in batches to manage API calls and context window limits\n",
    "    num_batches = (len(texts_to_process) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(texts_to_process), batch_size):\n",
    "        batch_series = texts_to_process[i:i + batch_size]\n",
    "        \n",
    "        # Convert the batch Series to a dictionary, using the original DataFrame index as the key.\n",
    "        # This is critical for mapping the results back correctly.\n",
    "        batch_dict = batch_series.to_dict()\n",
    "        \n",
    "        print(f\"Processing batch {i // batch_size + 1}/{num_batches} (Rows {min(batch_dict.keys())}-{max(batch_dict.keys())})...\")\n",
    "\n",
    "        # --- CHOOSE YOUR MODEL HERE ---\n",
    "        # Use 'gemini-2.5-flash-lite-preview-06-17' for super speed and super cost-effectiveness.\n",
    "        # Use 'gemini-2.5-flash' for speed and cost-effectiveness.\n",
    "        # Use 'gemini-2.5-pro' for higher quality results.\n",
    "        cleaned_batch_dict = clean_text_batch_json(batch_dict, system_prompt_content, model_name)\n",
    "            \n",
    "        if cleaned_batch_dict:\n",
    "            # Map the results back to the 'Cleaned Text' column using the original indices\n",
    "            for original_index, cleaned_text in cleaned_batch_dict.items():\n",
    "                # The keys from the returned dict are strings, so we convert them to int for indexing\n",
    "                df.loc[int(original_index), 'Cleaned Text'] = cleaned_text\n",
    "\n",
    "    # Drop the original column\n",
    "    df = df.drop(columns=['Segment Text'])\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nProcessing completed and saved to '{output_file}'\")\n",
    "    \n",
    "    \n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- DEFINE YOUR FILE PATHS HERE ---\n",
    "    # IMPORTANT: Replace this with the actual path to your input CSV file.\n",
    "    # Using a raw string (r'...') is helpful on Windows to avoid issues with backslashes.\n",
    "    input_csv_path = r'D:\\SOKM\\Testing\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed.csv'\n",
    "    \n",
    "    # Define the model name you want to use\n",
    "    model_name = \"gemini-2.5-pro\"  # Choose the model you want to use\n",
    "    \n",
    "    # --- Read the system prompt from a file ---\n",
    "    system_prompt_file = \"system_prompt_cleaning_v1.1.txt\"\n",
    "    system_prompt_content = read_instruction_file(system_prompt_file)\n",
    "    # system_prompt_content_tag contains only the version number, not the full content \n",
    "    # Eg: system_prompt_translate_v1.1 should lead to system_prompt_content_tag = \"v1.1\"\n",
    "    # This is useful for generating output file names.\n",
    "    system_prompt_content_tag = re.search(r\"v\\d+\\.\\d+\", system_prompt_file).group(0)\n",
    "    \n",
    "    \n",
    "    # Use regex to remove any special characters from the model name to create an output file name\n",
    "    model_tag = re.sub(r'[^a-zA-Z0-9]', '_', model_name)\n",
    "    \n",
    "    output_csv_path = input_csv_path.replace(\n",
    "        \".csv\", f\"_cleaned_{model_tag}_p{system_prompt_content_tag}.csv\"\n",
    "    )\n",
    "\n",
    "    # --- Run the process ---\n",
    "    # This batch size determines how many rows are sent in a single API request.\n",
    "    # You can adjust it based on the average length of your text and model context limits.\n",
    "    # A size of 25-100 is usually a good starting point.\n",
    "    process_csv(input_csv_path, output_csv_path, model_name, system_prompt_content, batch_size=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
