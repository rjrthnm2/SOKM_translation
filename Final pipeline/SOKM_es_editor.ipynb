{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d060c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  fix_translations.py  ·  v1.0\n",
    "#  ––– Corrects Spanish translations given English source lines\n",
    "# =============================================================================\n",
    "import os, re, time, json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ── LLM provider selection ---------------------------------------------------\n",
    "MODEL_PROVIDER = \"openai\"      # \"openai\"  or  \"gemini\"\n",
    "\n",
    "load_dotenv()                  # loads OPENAI_API_KEY or GOOGLE_API_KEY\n",
    "\n",
    "if MODEL_PROVIDER == \"openai\":\n",
    "    import openai\n",
    "    client = openai.OpenAI()\n",
    "    model_name = \"gpt-4o-mini\"       # pick your preferred model\n",
    "elif MODEL_PROVIDER == \"gemini\":\n",
    "    import google.generativeai as genai\n",
    "    genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    model_name = \"gemini-2.5-pro\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown provider: {MODEL_PROVIDER}\")\n",
    "\n",
    "# ── Optional EN–ES dictionary support ---------------------------------------\n",
    "ENABLE_DICTIONARY = True\n",
    "DICTIONARY_PATH   = \"en_es_dictionary.txt\"\n",
    "\n",
    "def load_en_es_dictionary(file_path:str) -> Dict[str, List[str]]:\n",
    "    \"\"\"Load a tab‑separated EN–ES dictionary into {head → [variants…]}.\"\"\"\n",
    "    d: Dict[str, List[str]] = defaultdict(list)\n",
    "    try:\n",
    "        with Path(file_path).open(encoding=\"utf8\") as f:\n",
    "            for raw in f:\n",
    "                if raw.startswith(\"#\") or not raw.strip():         # skip comments/blank\n",
    "                    continue\n",
    "                en, es, *rest = raw.rstrip(\"\\n\").split(\"\\t\")\n",
    "                d[en].append(\" \".join([es, *rest]).strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Dictionary not found at {file_path}. Continuing without it.\")\n",
    "    return d\n",
    "\n",
    "# --- spaCy matcher (keeps duplicates, boosts relevant dictionary terms) -----\n",
    "class DictionaryMatcher:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dictionary: Dict[str, List[str]],\n",
    "        model: str = \"en_core_web_sm\",\n",
    "        pos_focus: Set[str] = {\"NOUN\", \"ADJ\"},\n",
    "    ):\n",
    "        self.dictionary = dictionary\n",
    "        self.nlp  = spacy.load(model, disable=[\"ner\", \"parser\"])\n",
    "        self.posF = pos_focus\n",
    "\n",
    "        single, multi = set(), set()\n",
    "        for head in dictionary:\n",
    "            (multi if \" \" in head else single).add(head)\n",
    "\n",
    "        # lemma → head  (single‑word entries)\n",
    "        self.lemma2head = {h.lower(): h for h in single}\n",
    "\n",
    "        # multi‑word entries\n",
    "        self.phraser = PhraseMatcher(self.nlp.vocab, attr=\"LOWER\")\n",
    "        for phrase in multi:\n",
    "            self.phraser.add(phrase, [self.nlp.make_doc(phrase)])\n",
    "\n",
    "    def find_terms(self, text: str) -> Dict[str, List[str]]:\n",
    "        doc  = self.nlp(text)\n",
    "        hits = {}\n",
    "\n",
    "        # lemma‑based (single word)\n",
    "        for t in doc:\n",
    "            if t.pos_ in self.posF:\n",
    "                head = self.lemma2head.get(t.lemma_.lower())\n",
    "                if head:\n",
    "                    hits[head] = self.dictionary[head]\n",
    "\n",
    "        # multi‑word\n",
    "        for mid, s, e in self.phraser(doc):\n",
    "            head = self.nlp.vocab.strings[mid]\n",
    "            span = doc[s:e].text.lower()\n",
    "            if span == head.lower():\n",
    "                hits[head] = self.dictionary[head]\n",
    "        return hits\n",
    "\n",
    "# ── Prompt helpers -----------------------------------------------------------\n",
    "def read_file(path:str) -> str:\n",
    "    try:\n",
    "        return Path(path).read_text(encoding=\"utf8\").strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️  File not found: {path}\")\n",
    "        return \"\"\n",
    "\n",
    "def build_fix_prompt(\n",
    "    batch_pairs: dict,\n",
    "    system_prompt: str,\n",
    "    dictionary_block: str,\n",
    "    previous_context: list[str] | None = None,\n",
    "    provider: str = \"openai\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build the user prompt requesting corrected Spanish lines.\n",
    "    Returns a markdown prompt string; system_prompt goes separately.\n",
    "    \"\"\"\n",
    "    context_block = \"\"\n",
    "    if previous_context:\n",
    "        context_block = (\n",
    "            \"## Previous corrections (context)\\n\"\n",
    "            + \"\\n\".join(f\"- {line}\" for line in previous_context)\n",
    "            + \"\\n\\n\"\n",
    "        )\n",
    "\n",
    "    pairs_json = json.dumps(batch_pairs, indent=2, ensure_ascii=False)\n",
    "\n",
    "    if provider == \"openai\":\n",
    "        user_prompt = (\n",
    "            \"# Correction Task\\n\\n\"\n",
    "            \"You will receive a JSON object where each key maps to an object:\\n\"\n",
    "            '`{\"en\": \"...\", \"es\": \"...\"}\\'.\\n'\n",
    "            \"Return **one JSON object** with the **same keys** and the **corrected Spanish\\n\"\n",
    "            \"string** as each value. Keep strings unchanged when already correct.\\n\\n\"\n",
    "            \"Follow all style rules in the system prompt. Use the dictionary below\\n\"\n",
    "            \"only if it helps. Do not output anything except the JSON object.\\n\\n\"\n",
    "            f\"{context_block}\"\n",
    "            f\"{dictionary_block}\"\n",
    "            \"## Input JSON\\n```json\\n\" + pairs_json + \"\\n```\\n\"\n",
    "        )\n",
    "    else:      # gemini\n",
    "        user_prompt = (\n",
    "            \"Correct the Spanish lines in the JSON (same keys). \"\n",
    "            \"Return JSON only.\\n\\n\"\n",
    "            f\"{context_block}{dictionary_block}\"\n",
    "            \"Input:\\n\" + pairs_json\n",
    "        )\n",
    "    return user_prompt\n",
    "\n",
    "# ── LLM call -----------------------------------------------------------------\n",
    "def correct_batch_json(\n",
    "    batch_pairs: dict,\n",
    "    system_prompt: str,\n",
    "    dictionary_block: str,\n",
    "    previous_context: list[str] | None = None,\n",
    "    temperature: float = 0.0,\n",
    ") -> dict:\n",
    "    user_prompt = build_fix_prompt(\n",
    "        batch_pairs,\n",
    "        system_prompt,\n",
    "        dictionary_block,\n",
    "        previous_context,\n",
    "        provider=MODEL_PROVIDER,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        if MODEL_PROVIDER == \"openai\":\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=temperature,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\",   \"content\": user_prompt},\n",
    "                ],\n",
    "            )\n",
    "            return json.loads(response.choices[0].message.content)\n",
    "\n",
    "        else:  # gemini\n",
    "            model = genai.GenerativeModel(\n",
    "                model_name=model_name,\n",
    "                system_instruction=system_prompt,\n",
    "                generation_config={\n",
    "                    \"temperature\": temperature,\n",
    "                    \"response_mime_type\": \"application/json\",\n",
    "                },\n",
    "            )\n",
    "            resp = model.generate_content(user_prompt)\n",
    "            return json.loads(resp.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"⚠️  LLM call failed:\", e)\n",
    "        return {}\n",
    "\n",
    "# ── Pipeline -----------------------------------------------------------------\n",
    "def process_file(\n",
    "    input_file: str,\n",
    "    output_file: str,\n",
    "    system_prompt: str,\n",
    "    dictionary: Dict[str, List[str]] | None,\n",
    "    matcher: DictionaryMatcher | None,\n",
    "    batch_size: int = 40,\n",
    "):\n",
    "    df = pd.read_csv(input_file)\n",
    "    if not {\"Segment Text\", \"Translated Text\"}.issubset(df.columns):\n",
    "        raise ValueError(\"CSV must have 'Segment Text' and 'Translated Text' columns.\")\n",
    "\n",
    "    corrected_col = \"Translated Text (corrected)\"\n",
    "    df[corrected_col] = pd.NA\n",
    "\n",
    "    pairs_series = df[[\"Segment Text\", \"Translated Text\"]].dropna().to_dict(\"index\")\n",
    "    ids = list(pairs_series.keys())\n",
    "\n",
    "    previous_context: list[str] = []\n",
    "\n",
    "    for b in range(0, len(ids), batch_size):\n",
    "        batch_ids   = ids[b:b+batch_size]\n",
    "        batch_pairs = {\n",
    "            str(i): {\"en\": pairs_series[i][\"Segment Text\"],\n",
    "                     \"es\": pairs_series[i][\"Translated Text\"]}\n",
    "            for i in batch_ids\n",
    "        }\n",
    "\n",
    "        # build dictionary context\n",
    "        if matcher:\n",
    "            hits = matcher.find_terms(\" \".join(pairs_series[i][\"Segment Text\"]\n",
    "                                               for i in batch_ids))\n",
    "        elif dictionary:\n",
    "            hits = {k: v for k, v in dictionary.items()\n",
    "                    if any(re.search(rf\"\\b{k}\\b\", pairs_series[i][\"Segment Text\"], re.I)\n",
    "                           for i in batch_ids)}\n",
    "        else:\n",
    "            hits = {}\n",
    "\n",
    "        dict_block = \"\"\n",
    "        if hits:\n",
    "            dict_block = (\n",
    "                \"## Dictionary Context\\n\"\n",
    "                + \"\\n\".join(f\"• {k} → {' | '.join(v)}\" for k, v in hits.items())\n",
    "                + \"\\n\\n\"\n",
    "            )\n",
    "\n",
    "        print(f\"• Processing batch {b//batch_size + 1}\")\n",
    "        fixed = correct_batch_json(\n",
    "            batch_pairs,\n",
    "            system_prompt,\n",
    "            dict_block,\n",
    "            previous_context=previous_context,\n",
    "        )\n",
    "\n",
    "        # commit to dataframe\n",
    "        for i in batch_ids:\n",
    "            if str(i) in fixed:\n",
    "                df.loc[i, corrected_col] = fixed[str(i)]\n",
    "\n",
    "        # save some of this batch as rolling context\n",
    "        previous_context = [fixed[str(i)] for i in batch_ids if str(i) in fixed][:max(1, len(batch_ids)//2)]\n",
    "\n",
    "    df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅  Finished → {output_file}\")\n",
    "\n",
    "# ── Main entry ---------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # ‣‣‣ Adjust these paths --------------------------------------------------\n",
    "    input_csv_path   = r\"D:\\SOKM\\11 Identity 2\\identity_unit_translations.csv\"\n",
    "    system_prompt_file = \"system_prompt_fix_v1.0.txt\"      # your 'Span‑Eng Fixer' prompt\n",
    "    output_csv_path  = input_csv_path.replace(\".csv\", \"_corrected.csv\")\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    sys_prompt = read_file(system_prompt_file)\n",
    "    en_es_dict = load_en_es_dictionary(DICTIONARY_PATH) if ENABLE_DICTIONARY else {}\n",
    "    matcher    = DictionaryMatcher(en_es_dict) if ENABLE_DICTIONARY else None\n",
    "\n",
    "    process_file(\n",
    "        input_file=input_csv_path,\n",
    "        output_file=output_csv_path,\n",
    "        system_prompt=sys_prompt,\n",
    "        dictionary=en_es_dict,\n",
    "        matcher=matcher,\n",
    "        batch_size=30,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
