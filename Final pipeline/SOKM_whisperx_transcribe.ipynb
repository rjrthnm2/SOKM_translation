{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4daf8bb",
   "metadata": {},
   "source": [
    "Using whisperx to transcribe the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75081f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import whisperx\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import csv\n",
    "import json\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32= False\n",
    "torch.backends.cudnn.allow_tf32= False\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "834e379e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu118\n",
      "CUDA version used to build PyTorch: 11.8\n",
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version used to build PyTorch:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd71fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "\n",
    "# Step 1: Extract Audio from Video\n",
    "def extract_audio_from_video(video_file: str, audio_file: str):\n",
    "    print(f\"Starting audio extraction from video: {video_file}\")\n",
    "    subprocess.run([\n",
    "        \"ffmpeg\",\n",
    "        \"-i\", video_file,\n",
    "        \"-vn\",  # no video, only audio\n",
    "        \"-acodec\", \"pcm_s16le\",  # audio codec (WAV format)\n",
    "        \"-ar\", \"16000\",  # sample rate\n",
    "        \"-ac\", \"1\",  # number of audio channels\n",
    "        audio_file\n",
    "    ], check=True)\n",
    "    print(f\"Audio extraction completed. Audio saved to: {audio_file}\")\n",
    "\n",
    "# Step 2: Transcribe Audio with WhisperX\n",
    "def transcribe_audio_with_whisperx(audio_file: str):\n",
    "    print(f\"Starting transcription of audio file: {audio_file}\")\n",
    "    device = \"cuda\"\n",
    "    batch_size = 6\n",
    "    compute_type = \"float16\"\n",
    "    model = whisperx.load_model(\"large-v3\", device, vad_method=\"silero\", compute_type=compute_type,language='en')\n",
    "    # model = whisperx.load_model(\"large-v3\", device, compute_type=compute_type,language='en')\n",
    "    audio = whisperx.load_audio(audio_file)\n",
    "    transcription_result = model.transcribe(audio, batch_size=batch_size)\n",
    "    print(f\"Transcription complete. Number of segments: {len(transcription_result['segments'])}\")\n",
    "    model_a, metadata = whisperx.load_align_model(language_code=transcription_result[\"language\"], device=device)\n",
    "    aligned_result = whisperx.align(transcription_result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "    print(\"Alignment complete.\")\n",
    "    # diarize_model = whisperx.DiarizationPipeline(model_name=\"pyannote/speaker-diarization-3.1\",use_auth_token=\"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\", device=device)\n",
    "    # # add min/max number of speakers if known\n",
    "    # diarize_segments = diarize_model(audio)\n",
    "    # # diarize_model(audio, num_speakers= total_speakers, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "    # diarize_result = whisperx.assign_word_speakers(diarize_segments, aligned_result)\n",
    "    # # print(result[\"segments\"]) # segments are now assigned speaker IDs\n",
    "    # print(\"Diarization complete.\")\n",
    "    del audio\n",
    "    gc.collect()\n",
    "    if 'torch' in globals():\n",
    "        torch.cuda.empty_cache()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if 'torch' in globals():\n",
    "        torch.cuda.empty_cache()\n",
    "    return aligned_result\n",
    "\n",
    "# Step 3: Generate SRT File\n",
    "def generate_srt_file(transcription_result, translations, srt_file_path: str):\n",
    "    print(f\"Generating SRT file: {srt_file_path}\")\n",
    "    with open(srt_file_path, \"w\", encoding=\"utf-8\") as srt_file:\n",
    "        for idx, (segment, text) in enumerate(zip(transcription_result[\"segments\"], translations)):\n",
    "            start_time = segment[\"start\"]\n",
    "            end_time = segment[\"end\"]\n",
    "\n",
    "            start_str = f\"{int(start_time // 3600):02}:{int((start_time % 3600) // 60):02}:{int(start_time % 60):02},{int((start_time % 1) * 1000):03}\"\n",
    "            end_str = f\"{int(end_time // 3600):02}:{int((end_time % 3600) // 60):02}:{int(end_time % 60):02},{int((end_time % 1) * 1000):03}\"\n",
    "\n",
    "            srt_file.write(f\"{idx + 1}\\n\")\n",
    "            srt_file.write(f\"{start_str} --> {end_str}\\n\")\n",
    "            srt_file.write(f\"{text}\\n\\n\")\n",
    "    print(f\"SRT file generation complete: {srt_file_path}\")\n",
    "\n",
    "    # Check for potential issues in segment timestamps\n",
    "    for idx, segment in enumerate(transcription_result[\"segments\"]):\n",
    "        if segment[\"end\"] <= segment[\"start\"]:\n",
    "            print(f\"Warning: Misaligned timestamps in segment {idx + 1}. Start time: {segment['start']}, End time: {segment['end']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d6c74f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the main function\n",
    "\n",
    "video_file = r\"D:\\SOKM\\11 Identity 2 SoKM 2024 - 2025\\11 Identity 2 SoKM 2024 - 2025.mp4\"\n",
    "base_filename, _ = os.path.splitext(video_file)\n",
    "audio_file = f\"{base_filename}_audio.wav\"\n",
    "english_srt_file_path = f\"{base_filename}_transcript_english.srt\"\n",
    "spanish_srt_file_path = f\"{base_filename}_transcript_spanish.srt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1883c976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Extracting audio from video.\n",
      "Starting audio extraction from video: D:\\SOKM\\11 Identity 2 SoKM 2024 - 2025\\11 Identity 2 SoKM 2024 - 2025.mp4\n",
      "Audio extraction completed. Audio saved to: D:\\SOKM\\11 Identity 2 SoKM 2024 - 2025\\11 Identity 2 SoKM 2024 - 2025_audio.wav\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract audio from video\n",
    "print(\"Step 1: Extracting audio from video.\")\n",
    "extract_audio_from_video(video_file, audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f3688a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Transcribing audio with WhisperX.\n",
      "Starting transcription of audio file: D:\\SOKM\\11 Identity 2 SoKM 2024 - 2025\\11 Identity 2 SoKM 2024 - 2025_audio.wav\n",
      ">>Performing voice activity detection using Silero...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\robin/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription complete. Number of segments: 89\n",
      "Alignment complete.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Transcribe audio with WhisperX\n",
    "print(\"Step 2: Transcribing audio with WhisperX.\")\n",
    "transcription_result = transcribe_audio_with_whisperx(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06df4ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to D:\\SOKM\\11 Identity 2 SoKM 2024 - 2025\\11 Identity 2 SoKM 2024 - 2025_audio_english_v3s.csv\n"
     ]
    }
   ],
   "source": [
    "#Step 2b: storing temporary csv output\n",
    "# Specify the filename for the CSV file\n",
    "csv_filename = audio_file.rsplit('.',1)[0] +'_english_v3s.csv'\n",
    "\n",
    "# Open a CSV file to write to\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "#     fieldnames = ['Segment Start', 'Segment End', 'Segment Text','Speaker']\n",
    "    fieldnames = ['Segment Start', 'Segment End', 'Segment Text']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for segment in transcription_result[\"segments\"]:\n",
    "        segment_start = segment['start']\n",
    "        segment_end = segment['end']\n",
    "        segment_text = segment['text']\n",
    "        # # Check if the words list is non-empty and if the first word has a 'speaker' key\n",
    "        # if segment.get('words') and 'speaker' in segment['words'][0]:\n",
    "        #         segment_speaker = segment['words'][0]['speaker']\n",
    "        # else:\n",
    "        #         speaker_counts = {}  # Dictionary to count occurrences of each speaker\n",
    "        \n",
    "        #         for word in segment.get('words', []):  # Ensure 'words' exists\n",
    "        #                 speaker = word.get('speaker')\n",
    "        #                 if speaker:  # Only count non-empty speaker values\n",
    "        #                         speaker_counts[speaker] = speaker_counts.get(speaker, 0) + 1\n",
    "                        \n",
    "        #         # Determine the most frequent speaker\n",
    "        #         if speaker_counts:\n",
    "        #                 segment_speaker = max(speaker_counts, key=speaker_counts.get)\n",
    "        #         else:\n",
    "        #                 segment_speaker = 'Unknown'  # Default if no speakers exist\n",
    "        writer.writerow({\n",
    "                'Segment Start': segment_start,\n",
    "                'Segment End': segment_end,\n",
    "                'Segment Text': segment_text,\n",
    "                # 'Speaker': segment_speaker\n",
    "        })\n",
    "\n",
    "print(f\"Data successfully written to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c46eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2c: storing temporary json output\n",
    "json_filename = audio_file.rsplit('.',1)[0] +'_english_v3s.json'\n",
    "with open(json_filename, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(transcription_result, json_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e28be8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Generating English SRT file.\n",
      "Generating SRT file: D:\\SOKM\\11 Identity 2 SoKM 2024 - 2025\\11 Identity 2 SoKM 2024 - 2025_transcript_english.srt\n",
      "SRT file generation complete: D:\\SOKM\\11 Identity 2 SoKM 2024 - 2025\\11 Identity 2 SoKM 2024 - 2025_transcript_english.srt\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Generate English SRT file\n",
    "print(\"Step 3: Generating English SRT file.\")\n",
    "english_translations = [segment[\"text\"] for segment in transcription_result[\"segments\"]]\n",
    "generate_srt_file(transcription_result, english_translations, english_srt_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
