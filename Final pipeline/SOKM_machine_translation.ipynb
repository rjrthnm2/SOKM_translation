{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a74d994f",
   "metadata": {},
   "source": [
    "OpenAI/Gemini translation with or without dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf01e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os, re, time, json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set           # keep for type hints\n",
    "\n",
    "# >>> NEW ─────────────────────────────────────────────────────────────────────\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# --- Configuration ----------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# Choose which model provider to use\n",
    "MODEL_PROVIDER = \"openai\"  # \"openai\" or \"gemini\"\n",
    "\n",
    "# Initialize clients based on provider\n",
    "if MODEL_PROVIDER == \"openai\":\n",
    "    if \"OPENAI_API_KEY\" not in os.environ:\n",
    "        raise ValueError(\"OPENAI_API_KEY not found in .env file or environment variables.\")\n",
    "    import openai\n",
    "    client = openai.OpenAI()\n",
    "elif MODEL_PROVIDER == \"gemini\":\n",
    "    import google.generativeai as genai\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GOOGLE_API_KEY not found in .env file. Please add it.\")\n",
    "    genai.configure(api_key=api_key)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model provider: {MODEL_PROVIDER}\")\n",
    "\n",
    "ENABLE_DICTIONARY = True\n",
    "DICTIONARY_PATH   = \"en_es_dictionary.txt\"\n",
    "\n",
    "\n",
    "# ── Utility ­­­functions ­­­(unchanged) --------------------------------------\n",
    "def read_instruction_file(file_path: str) -> str:\n",
    "    \"\"\"Reads the system-prompt file (returns empty string on failure).\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: prompt file not found at {file_path}. Using empty prompt.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def load_en_es_dictionary(file_path: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Read a tab-separated EN–ES dictionary, preserving duplicate headwords.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, list[str]]\n",
    "        {headword → [full_entry1, full_entry2, …]}\n",
    "    \"\"\"\n",
    "    dictionary: Dict[str, List[str]] = defaultdict(list)\n",
    "    \n",
    "    try:\n",
    "        with Path(file_path).open(encoding=\"utf-8\") as f:\n",
    "            for raw in f:\n",
    "                if raw.startswith(\"#\") or not raw.strip():\n",
    "                    continue\n",
    "\n",
    "                cols = raw.rstrip(\"\\n\").split(\"\\t\")\n",
    "                if len(cols) < 2:\n",
    "                    continue  # guard against malformed lines\n",
    "\n",
    "                en_term, es_term = cols[0], cols[1]\n",
    "                details = \"\\t\".join(cols[2:])\n",
    "                dictionary[en_term].append(f\"{es_term} {details}\".strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Dictionary file not found at {file_path}. Proceeding without it.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading dictionary {file_path}: {e}\")\n",
    "    return dictionary\n",
    "\n",
    "# >>> NEW — Missing function definition (used as fallback) ------------------\n",
    "def find_relevant_dictionary_terms(text: str, dictionary: dict) -> dict:\n",
    "    \"\"\"Finds relevant terms from the dictionary in the given text.\"\"\"\n",
    "    relevant_terms = {}\n",
    "    for term in dictionary:\n",
    "        if re.search(r'\\b' + re.escape(term) + r'\\b', text, re.IGNORECASE):\n",
    "            relevant_terms[term] = dictionary[term]\n",
    "    return relevant_terms\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# >>> NEW ─────────────────────────────────────────────────────────────────────\n",
    "class DictionaryMatcher:\n",
    "    \"\"\"\n",
    "    Wraps a spaCy tokeniser + PhraseMatcher; keeps duplicates.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dictionary: Dict[str, List[str]],\n",
    "        # model: str = \"en_core_web_sm\",\n",
    "        model: str = \"en_core_web_trf\",  # transformer-based model for better accuracy\n",
    "        pos_focus: Set[str] = {\"NOUN\", \"ADJ\"},\n",
    "    ) -> None:\n",
    "        self.dictionary = dictionary\n",
    "        self.nlp  = spacy.load(model, disable=[\"ner\", \"parser\"])\n",
    "        self.posF = pos_focus\n",
    "\n",
    "        single, multi = set(), set()\n",
    "        for h in dictionary:\n",
    "            (multi if \" \" in h else single).add(h)\n",
    "\n",
    "        # fast lemma map for single-word entries\n",
    "        self.lemma2head = {h.lower(): h for h in single}\n",
    "\n",
    "        # PhraseMatcher for multi-word entries\n",
    "        self.phraser = PhraseMatcher(self.nlp.vocab, attr=\"LOWER\")\n",
    "        for phrase in multi:\n",
    "            self.phraser.add(phrase, [self.nlp.make_doc(phrase)])\n",
    "\n",
    "    def find_terms(self, text: str) -> Dict[str, List[str]]:\n",
    "        doc  = self.nlp(text)\n",
    "        hits = {}\n",
    "\n",
    "        # lemma-based single words\n",
    "        for t in doc:\n",
    "            if t.pos_ in self.posF:\n",
    "                head = self.lemma2head.get(t.lemma_.lower())\n",
    "                if head:\n",
    "                    hits[head] = self.dictionary[head]\n",
    "\n",
    "        # multi-word phrases\n",
    "        for mid, s, e in self.phraser(doc):\n",
    "            head = self.nlp.vocab.strings[mid]          # original headword\n",
    "            span = doc[s:e].text.lower()\n",
    "            if span == head.lower():\n",
    "                hits[head] = self.dictionary[head]\n",
    "\n",
    "        return hits\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def translate_text_batch_json(\n",
    "    batch_dict: dict,\n",
    "    system_prompt_content: str,\n",
    "    dictionary_context: str,\n",
    "    model_name: str = \"gpt-4.1\",\n",
    "    temperature: float = 1,\n",
    "    context_translations: list[str] | None = None,\n",
    "    provider: str = \"openai\",\n",
    ") -> dict:\n",
    "\n",
    "    # Build the prompt --------------------------------------------------------\n",
    "    context_block = \"\"\n",
    "    if context_translations:\n",
    "        context_block = (\n",
    "            \"## Previous Context\\n\\n\"\n",
    "            \"**Previous translated lines** (do NOT modify these):\\n\\n\"\n",
    "            + \"\\n\".join(f\"- {line}\" for line in context_translations) + \"\\n\\n\"\n",
    "        )\n",
    "\n",
    "    dictionary_context_block = \"\"\n",
    "    if dictionary_context:\n",
    "        dictionary_context_block = (\n",
    "            \"## Dictionary Context\\n\\n\"\n",
    "            \"**Format**: English term → Spanish translation(s)\\n\"\n",
    "            \"**Note**: Multiple options are separated by ` | `\\n\\n\"\n",
    "            f\"{dictionary_context}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    texts_json = json.dumps(batch_dict, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    if provider == \"openai\":\n",
    "        user_prompt = (\n",
    "            \"# Translation Task\\n\\n\"\n",
    "            \"Please translate each value in the JSON object from English to Spanish \"\n",
    "            \"following the rules in the system prompt. Return exactly one JSON object \"\n",
    "            \"with identical keys and the translated Spanish text as values.\\n\\n\"\n",
    "            \n",
    "            \"## Translation Guidelines\\n\\n\"\n",
    "            \"1. **Follow ALL rules** specified in the system prompt\\n\"\n",
    "            \"2. **Use dictionary entries** below for additional context and accuracy\\n\"\n",
    "            \"3. **Dictionary format**: English term → Spanish translation(s)\\n\"\n",
    "            \"4. **Multiple variants**: When Spanish variants are shown (separated by `|`), choose the most appropriate one for context\\n\"\n",
    "            \"5. **Priority**: Dictionary suggestions should complement, not override, system prompt rules\\n\\n\"\n",
    "            f\"{dictionary_context_block}\"\n",
    "            \"## Input JSON to Translate\\n\\n\"\n",
    "            \"```json\\n\" + texts_json + \"\\n```\\n\\n\"\n",
    "            \n",
    "            # \"## Required Output Format\\n\\n\"\n",
    "            # \"**Return a valid JSON object** with:\\n\"\n",
    "            # \"- Same keys as the input\\n\"\n",
    "            # \"- Spanish translations as values\\n\"\n",
    "            # \"- No other text, explanations, or formatting outside the JSON object\\n\\n\"\n",
    "            # \"**Example output format:**\\n\"\n",
    "            # \"```json\\n\"\n",
    "            # '{\"0\": \"Spanish translation here\", \"1\": \"Another Spanish translation\"}\\n'\n",
    "            # \"```\"\n",
    "        )\n",
    "    else:  # gemini\n",
    "        user_prompt = (\n",
    "            \"Please translate each value in the JSON object from English to Spanish \"\n",
    "            \"following the rules in the system prompt. Return exactly one JSON object \"\n",
    "            \"with identical keys.\\n\\n\"\n",
    "            f\"{context_block}\"\n",
    "            f\"{dictionary_context_block}\"\n",
    "            \"Input Texts:\\n\" + texts_json\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        if provider == \"openai\":\n",
    "            # --- Call the OpenAI API ---\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=temperature,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt_content},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            print(\"user prompt:\", user_prompt)\n",
    "            response_content = response.choices[0].message.content\n",
    "            return json.loads(response_content)\n",
    "        \n",
    "        elif provider == \"gemini\":\n",
    "            # --- Call the Gemini API ---\n",
    "            model = genai.GenerativeModel(\n",
    "                model_name=model_name,\n",
    "                system_instruction=system_prompt_content,\n",
    "                generation_config={\n",
    "                    \"temperature\": temperature,\n",
    "                    \"response_mime_type\": \"application/json\",\n",
    "                },\n",
    "            )\n",
    "            \n",
    "            response = model.generate_content(user_prompt)\n",
    "            return json.loads(response.text)\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Failed to decode JSON from model response. Error: {e}\")\n",
    "        if 'response_content' in locals():\n",
    "            print(f\"Model response was:\\n---\\n{response_content}\\n---\")\n",
    "        elif 'response' in locals():\n",
    "            print(f\"Model response was:\\n---\\n{response.text}\\n---\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        if provider == \"openai\" and hasattr(e, '__class__') and 'openai' in str(e.__class__):\n",
    "            print(f\"An OpenAI API error occurred: {e}\")\n",
    "        else:\n",
    "            print(f\"An API error occurred: {e}\")\n",
    "        time.sleep(1)\n",
    "        return {}\n",
    "\n",
    "def process_csv_with_llm(\n",
    "    input_file: str,\n",
    "    output_file: str,\n",
    "    model_name: str,\n",
    "    system_prompt_content: str,\n",
    "    dictionary: dict | None,\n",
    "    matcher: \"DictionaryMatcher | None\",\n",
    "    provider: str = \"openai\",\n",
    "    batch_size: int = 50,\n",
    "    text_column: str = \"Segment Text\",  # ← new parameter\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads a CSV, translates the specified text column in batches,\n",
    "    and writes a new CSV with 'Translated Text'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: input file not found: {input_file}\")\n",
    "        return\n",
    "\n",
    "    # Check for the specified column name\n",
    "    if text_column not in df.columns:\n",
    "        # Try alternative column names\n",
    "        alternative_columns = [\"Segment Text\", \"Cleaned Text\"]\n",
    "        found_column = None\n",
    "        for alt_col in alternative_columns:\n",
    "            if alt_col in df.columns:\n",
    "                found_column = alt_col\n",
    "                break\n",
    "        \n",
    "        if found_column:\n",
    "            print(f\"'{text_column}' not found, using '{found_column}' instead.\")\n",
    "            text_column = found_column\n",
    "        else:\n",
    "            print(f\"Error: Neither '{text_column}' nor alternative columns {alternative_columns} found.\")\n",
    "            print(f\"Available columns: {list(df.columns)}\")\n",
    "            return\n",
    "\n",
    "    df[\"Translated Text\"] = pd.NA\n",
    "    texts_series = df[text_column].dropna()  # ← use the determined column\n",
    "    num_batches = (len(texts_series) + batch_size - 1) // batch_size\n",
    "    previous_context_lines: list[str] = []\n",
    "\n",
    "    for start in range(0, len(texts_series), batch_size):\n",
    "        batch_series = texts_series[start:start + batch_size]\n",
    "        batch_dict   = batch_series.to_dict()\n",
    "\n",
    "        # >>> CHANGED — use spaCy matcher if present\n",
    "        if matcher is not None:\n",
    "            rel_terms = matcher.find_terms(\" \".join(batch_series.astype(str)))\n",
    "        elif dictionary:\n",
    "            rel_terms = find_relevant_dictionary_terms(\" \".join(batch_series.astype(str)),\n",
    "                                                       dictionary)\n",
    "        else:\n",
    "            rel_terms = {}\n",
    "\n",
    "        # Format dictionary context to show all variants more clearly\n",
    "        dict_lines = []\n",
    "        for k, v in rel_terms.items():\n",
    "            if isinstance(v, list):\n",
    "                # Show all variants for this term, cleaned up\n",
    "                variants = \" | \".join(entry.strip() for entry in v)\n",
    "                dict_lines.append(f\"• {k} → {variants}\")\n",
    "            else:\n",
    "                dict_lines.append(f\"• {k} → {v}\")\n",
    "        dictionary_context = \"\\n\".join(dict_lines)\n",
    "\n",
    "        print(f\"Batch {start // batch_size + 1}/{num_batches} \"\n",
    "              f\"(rows {batch_series.index.min()}–{batch_series.index.max()})\")\n",
    "        # show what was found\n",
    "        # print(\"  term_hits ➜\", list(rel_terms) or \"∅\")\n",
    "\n",
    "        translated = translate_text_batch_json(\n",
    "            batch_dict=batch_dict,\n",
    "            system_prompt_content=system_prompt_content,\n",
    "            dictionary_context=dictionary_context,\n",
    "            model_name=model_name,\n",
    "            temperature=1,\n",
    "            context_translations=previous_context_lines,\n",
    "            provider=provider,                    # ← pass provider\n",
    "        )\n",
    "\n",
    "        if translated:\n",
    "            for idx, es in translated.items():\n",
    "                df.loc[int(idx), \"Translated Text\"] = es\n",
    "            ordered_ids = list(batch_series.index)\n",
    "            ordered_es  = [translated[str(i)] for i in ordered_ids if str(i) in translated]\n",
    "            previous_context_lines = ordered_es[: max(1, len(ordered_es)//2)]\n",
    "\n",
    "    df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\n✅  Translation finished → {output_file}\")\n",
    "\n",
    "\n",
    "# ── Main --------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_path  = r\"D:\\SOKM\\11 Identity 2 SoKM 2024 - 2025\\11 Identity 2 SoKM 2024 - 2025_transcript_english_SE_br_converted_cleaned_gpt_4_1.csv\"\n",
    "    system_prompt_file = \"system_prompt_translate_v2.0.txt\"\n",
    "    \n",
    "    # Automatically determine text column based on filename\n",
    "    text_column = \"Cleaned Text\" if \"_cleaned_\" in input_csv_path.lower() else \"Segment Text\"\n",
    "    print(f\"Using text column: '{text_column}'\")\n",
    "    \n",
    "    # Model configuration - change these to switch between providers\n",
    "    if MODEL_PROVIDER == \"openai\":\n",
    "        model_name = \"gpt-5\"\n",
    "    elif MODEL_PROVIDER == \"gemini\":\n",
    "        model_name = \"gemini-2.5-pro\"\n",
    "    \n",
    "    system_prompt_content = read_instruction_file(system_prompt_file)\n",
    "\n",
    "    en_es_dictionary = (load_en_es_dictionary(DICTIONARY_PATH)\n",
    "                        if ENABLE_DICTIONARY else {})\n",
    "\n",
    "    # >>> NEW — build matcher once\n",
    "    matcher = (DictionaryMatcher(en_es_dictionary)\n",
    "               if ENABLE_DICTIONARY else None)\n",
    "\n",
    "    model_tag = re.sub(r\"[^A-Za-z0-9]\", \"_\", model_name)\n",
    "    # system_prompt_content_tag contains only the version number, not the full content \n",
    "    # Eg: system_prompt_translate_v1.1 should lead to system_prompt_content_tag = \"v1.1\"\n",
    "    # This is useful for generating output file names.\n",
    "    system_prompt_content_tag = re.search(r\"v\\d+\\.\\d+\", system_prompt_file).group(0)\n",
    "    dictionary_tag = \"d_\" if ENABLE_DICTIONARY else \"\"\n",
    "    provider_tag = f\"{MODEL_PROVIDER}_\"\n",
    "    output_csv_path = input_csv_path.replace(\n",
    "        \".csv\", f\"_translated_{dictionary_tag}{provider_tag}{model_tag}_p{system_prompt_content_tag}.csv\"\n",
    "    )\n",
    "    \n",
    "    process_csv_with_llm(\n",
    "        input_file=input_csv_path,\n",
    "        output_file=output_csv_path,\n",
    "        model_name=model_name,\n",
    "        system_prompt_content=system_prompt_content,\n",
    "        dictionary=en_es_dictionary,\n",
    "        matcher=matcher,                 # ← pass matcher\n",
    "        provider=MODEL_PROVIDER,         # ← pass provider\n",
    "        batch_size=50,\n",
    "        text_column=text_column,         # ← pass the determined column\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1b9c9",
   "metadata": {},
   "source": [
    "Evaluating translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b332838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import jiwer\n",
    "from jiwer import wer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "\"\"\"Translation‑evaluation utility\n",
    "\n",
    "Compares multiple machine‑translation (MT) outputs against a native‑speaker\n",
    "reference and plots average BLEU, METEOR, WER, and COMET scores.\n",
    "\n",
    "Assumptions\n",
    "-----------\n",
    "* **Reference file** contains the canonical translation plus the English source\n",
    "  text:\n",
    "    D:\\SOKM\\01 Introduction SoKM 2024 - 2025 transcript_translation_Proofread.csv\n",
    "  ├─ Segment Text              (English source)\n",
    "  └─ Translation_nativespeaker (reference)\n",
    "\n",
    "* **MT outputs** live in *Testing* with naming pattern\n",
    "    4k_audio_english_fixed_testtiny_translated_<model>.csv\n",
    "  └─ Translated Text           (model hypothesis)\n",
    "  They do **not** repeat the Segment Text column; we align by row order.\n",
    "\n",
    "The script discovers all MT files automatically, computes sentence‑level scores\n",
    "in one pass per model, saves both row‑wise and model‑wise CSVs next to the\n",
    "reference, and draws two grouped bar plots.\n",
    "\"\"\"\n",
    "\n",
    "# ───────────────────────────── paths & columns ──────────────────────────────\n",
    "# REFERENCE_CSV = (\n",
    "#     r\"D:\\SOKM\\Testing\\01 Introduction SoKM 2024 - 2025 \"\n",
    "#     r\"transcript_translation_Proofread.csv\"\n",
    "# )\n",
    "# MT_PATTERN = (\n",
    "#     r\"D:\\SOKM\\Testing\\01 Introduction SoKM 2024 - 2025 \"\n",
    "#     r\"4k_audio_english_fixed_translated_*.csv\"\n",
    "# )\n",
    "REFERENCE_CSV = (\n",
    "    r\"D:\\SOKM\\Testing\\01 Introduction SoKM 2024 - 2025 \"\n",
    "    r\"transcript_translation_Proofread.csv\"\n",
    ")\n",
    "MT_PATTERN = (\n",
    "    r\"D:\\SOKM\\Testing\\01 Introduction SoKM 2024 - 2025 \"\n",
    "    r\"4k_audio_english_fixed_translated_*.csv\"\n",
    ")\n",
    "SRC_COL = \"Segment Text\"\n",
    "REF_COL = \"Translation_nativespeaker\"\n",
    "HYP_COL = \"Translated Text\"\n",
    "ENC = \"cp1252\"\n",
    "\n",
    "# ───────────────────────────── metric helpers ───────────────────────────────\n",
    "_smooth = SmoothingFunction().method1\n",
    "\n",
    "def bleu(r: str, h: str) -> float:\n",
    "    return sentence_bleu([word_tokenize(r.lower())],\n",
    "                         word_tokenize(h.lower()),\n",
    "                         smoothing_function=_smooth)\n",
    "\n",
    "def meteor(r: str, h: str) -> float:\n",
    "    return meteor_score([word_tokenize(r.lower())],\n",
    "                        word_tokenize(h.lower()))\n",
    "\n",
    "def wer_acc(r: str, h: str) -> float:\n",
    "    transforms = jiwer.Compose(\n",
    "        [\n",
    "            jiwer.ExpandCommonEnglishContractions(),\n",
    "            jiwer.RemoveEmptyStrings(),\n",
    "            jiwer.ToLowerCase(),\n",
    "            jiwer.RemoveMultipleSpaces(),\n",
    "            jiwer.Strip(),\n",
    "            jiwer.RemovePunctuation(),\n",
    "            jiwer.ReduceToListOfListOfWords(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    wer = jiwer.wer(\n",
    "                r,\n",
    "                h,\n",
    "                reference_transform=transforms,\n",
    "                hypothesis_transform=transforms,\n",
    "                )\n",
    "    \n",
    "    return wer  # accuracy style (higher = better)\n",
    "\n",
    "# ───────────────────────────── load reference ───────────────────────────────\n",
    "ref_df = pd.read_csv(REFERENCE_CSV, encoding=ENC, usecols=[SRC_COL, REF_COL])\n",
    "source_texts = ref_df[SRC_COL].tolist()\n",
    "references   = ref_df[REF_COL].tolist()\n",
    "num_segments = len(ref_df)\n",
    "\n",
    "# ───────────────────────────── load COMET once ─────────────────────────────\n",
    "print(\"Loading COMET …\", end=\" \")\n",
    "ckpt = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "comet = load_from_checkpoint(ckpt)\n",
    "USE_GPU = int(torch.cuda.is_available())\n",
    "print(\"done.\")\n",
    "\n",
    "# ───────────────────────────── process each MT file ─────────────────────────\n",
    "summary_rows, row_metrics_frames = [], []\n",
    "\n",
    "for mt_csv in glob.glob(MT_PATTERN):\n",
    "    model_name = re.search(r\"translated_(.*?).csv\", Path(mt_csv).name).group(1)\n",
    "    print(f\"Evaluating {model_name} …\")\n",
    "\n",
    "    hyp_df = pd.read_csv(mt_csv, encoding=ENC, usecols=[HYP_COL])\n",
    "    hypotheses = hyp_df[HYP_COL].tolist()\n",
    "\n",
    "    if len(hypotheses) != num_segments:\n",
    "        raise ValueError(\n",
    "            f\"Row mismatch: {model_name} has {len(hypotheses)} rows, \"\n",
    "            f\"reference has {num_segments}.\")\n",
    "    \n",
    "    # sentence‑level metrics\n",
    "    bleu_scores, meteor_scores, wer_scores, comet_batch = [], [], [], []\n",
    "\n",
    "    for src, ref, hyp in zip(source_texts, references, hypotheses):\n",
    "        bleu_scores.append(bleu(ref, hyp))\n",
    "        meteor_scores.append(meteor(ref, hyp))\n",
    "        wer_scores.append(wer_acc(ref, hyp))\n",
    "        comet_batch.append({\"src\": src, \"mt\": hyp, \"ref\": ref})\n",
    "\n",
    "    comet_scores = comet.predict(comet_batch, batch_size=16, gpus=USE_GPU).scores\n",
    "\n",
    "    # collect row‑level output\n",
    "    frame = pd.DataFrame({\n",
    "        SRC_COL: source_texts,\n",
    "        REF_COL: references,\n",
    "        HYP_COL: hypotheses,\n",
    "        \"BLEU\": bleu_scores,\n",
    "        \"METEOR\": meteor_scores,\n",
    "        \"WER\": wer_scores,\n",
    "        \"COMET\": comet_scores,\n",
    "        \"model\": model_name,\n",
    "    })\n",
    "    row_metrics_frames.append(frame)\n",
    "\n",
    "    # model averages\n",
    "    summary_rows.append({\n",
    "        \"model\": model_name,\n",
    "        \"BLEU\": sum(bleu_scores)   / num_segments,\n",
    "        \"METEOR\": sum(meteor_scores) / num_segments,\n",
    "        \"WER\": sum(wer_scores)    / num_segments,\n",
    "        \"COMET\": sum(comet_scores)  / num_segments,\n",
    "    })\n",
    "\n",
    "print(\"All models evaluated.\\n\")\n",
    "\n",
    "# ───────────────────────────── save CSVs ────────────────────────────────────\n",
    "base = Path(REFERENCE_CSV).with_suffix(\"\")\n",
    "row_csv     = f\"{base}_row_metrics.csv\"\n",
    "summary_csv = f\"{base}_summary_metrics.csv\"\n",
    "\n",
    "pd.concat(row_metrics_frames, ignore_index=True).to_csv(row_csv, index=False,encoding=ENC)\n",
    "pd.DataFrame(summary_rows).to_csv(summary_csv, index=False, encoding=ENC)\n",
    "print(f\"Row‑level metrics → {row_csv}\\nModel summary     → {summary_csv}\\n\")\n",
    "\n",
    "# ───────────────────────────── plots ────────────────────────────────────────\n",
    "summary_long = pd.DataFrame(summary_rows).melt(\"model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 5))\n",
    "ax = sns.barplot(data=summary_long, x=\"model\", y=\"Score\", hue=\"Metric\")\n",
    "ax.set_title(\"Average Scores by Model\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax2 = sns.barplot(data=summary_long, x=\"Metric\", y=\"Score\", hue=\"model\")\n",
    "ax2.set_title(\"Average Scores by Metric\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
