{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0d3d42",
   "metadata": {},
   "source": [
    "Validating AI generated translation with expert translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from jiwer import wer\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculates the BLEU score for a single hypothesis against a reference,\n",
    "    using a smoothing function to avoid zero scores for short sentences.\n",
    "    - reference: The reference translation (string).\n",
    "    - hypothesis: The machine-generated translation (string).\n",
    "    - Returns: BLEU score (float).\n",
    "    \"\"\"\n",
    "    # NLTK expects a list of tokenized references and a tokenized hypothesis\n",
    "    reference_tokens = [word_tokenize(str(reference).lower())]\n",
    "    hypothesis_tokens = word_tokenize(str(hypothesis).lower())\n",
    "    \n",
    "    # Instantiate a smoothing function. Method1 is a simple 'add-epsilon' smoothing.\n",
    "    # This helps prevent scores of 0 for sentences with no higher-order n-gram matches.\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    \n",
    "    # Calculate BLEU score with the smoothing function\n",
    "    return sentence_bleu(\n",
    "        reference_tokens,\n",
    "        hypothesis_tokens,\n",
    "        smoothing_function=smoothing_function\n",
    "    )\n",
    "\n",
    "def calculate_meteor(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculates the METEOR score.\n",
    "    - reference: The reference translation (string).\n",
    "    - hypothesis: The machine-generated translation (string).\n",
    "    - Returns: METEOR score (float).\n",
    "    \"\"\"\n",
    "    # NLTK's meteor_score expects tokenized inputs\n",
    "    reference_tokens = word_tokenize(reference.lower())\n",
    "    hypothesis_tokens = word_tokenize(hypothesis.lower())\n",
    "    # Use a list for references as per meteor_score documentation, although we have one.\n",
    "    return meteor_score([reference_tokens], hypothesis_tokens)\n",
    "\n",
    "def calculate_wer(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculates 1-WER (Word Accuracy Rate) for consistency with other metrics.\n",
    "    - reference: The reference translation (string).\n",
    "    - hypothesis: The machine-generated translation (string).\n",
    "    - Returns: 1-WER score (float) where higher is better.\n",
    "    \"\"\"\n",
    "    return 1 - wer(reference, hypothesis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd95949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------  configuration  ------------------------------\n",
    "csv_file_path   = r\"D:\\SOKM\\01 Introduction SoKM 2024 - 2025 transcript_translation_Proofread.csv\"\n",
    "source_column   = \"Segment Text\"\n",
    "reference_column = \"Translation_nativespeaker\"\n",
    "\n",
    "# -----------------------------  load data  ----------------------------------\n",
    "df = pd.read_csv(csv_file_path, encoding=\"latin1\")\n",
    "\n",
    "# detect hypothesis columns\n",
    "hypothesis_columns = (\n",
    "    df.columns\n",
    "      .str.startswith(\"Translation_\") &\n",
    "    ~df.columns.isin([reference_column])\n",
    ")\n",
    "hypothesis_columns = df.columns[hypothesis_columns].tolist()\n",
    "\n",
    "if not hypothesis_columns:\n",
    "    raise ValueError(\"No hypothesis columns starting with 'Translation_' found.\")\n",
    "\n",
    "print(f\"Found hypothesis columns: {hypothesis_columns}\")\n",
    "\n",
    "# drop rows with missing data in any essential column\n",
    "essential_cols = [source_column, reference_column] + hypothesis_columns\n",
    "df.dropna(subset=essential_cols, inplace=True)\n",
    "print(f\"Processing {len(df)} rows after dropping NA values.\")\n",
    "\n",
    "# -----------------------------  COMET model  --------------------------------\n",
    "print(\"Loading COMET model …\")\n",
    "model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "comet_model = load_from_checkpoint(model_path)\n",
    "use_gpu = 1 if torch.cuda.is_available() else 0\n",
    "print(f\"COMET loaded (GPU={'yes' if use_gpu else 'no'}).\")\n",
    "\n",
    "# ---------------------------  metric calculation  ---------------------------\n",
    "summary_rows = []\n",
    "\n",
    "for hyp_col in hypothesis_columns:\n",
    "    print(f\"Calculating metrics for '{hyp_col}' …\")\n",
    "    # row-wise lists\n",
    "    bleu_scores   = []\n",
    "    meteor_scores = []\n",
    "    wer_scores    = []\n",
    "\n",
    "    # COMET needs dicts\n",
    "    comet_data = []\n",
    "\n",
    "    for src, ref, hyp in zip(df[source_column], df[reference_column], df[hyp_col]):\n",
    "        bleu_scores  .append(calculate_bleu(ref, hyp))\n",
    "        meteor_scores.append(calculate_meteor(ref, hyp))\n",
    "        wer_scores   .append(calculate_wer  (ref, hyp))\n",
    "        comet_data.append({\"src\": src, \"mt\": hyp, \"ref\": ref})\n",
    "\n",
    "    # COMET predictions in a single batch\n",
    "    comet_scores = comet_model.predict(comet_data,\n",
    "                                       batch_size=16,\n",
    "                                       gpus=use_gpu).scores\n",
    "\n",
    "    # -------------  attach row-level scores to df  -------------\n",
    "    df[f\"BLEU_{hyp_col}\"]   = bleu_scores\n",
    "    df[f\"METEOR_{hyp_col}\"] = meteor_scores\n",
    "    df[f\"1-WER_{hyp_col}\"]  = wer_scores  # Now contains 1-WER values\n",
    "    df[f\"COMET_{hyp_col}\"]  = comet_scores\n",
    "\n",
    "    # -------------  collect averages for summary  --------------\n",
    "    summary_rows.append({\n",
    "        \"hypothesis_column\": hyp_col,\n",
    "        \"avg_BLEU\"  : sum(bleu_scores)   / len(bleu_scores),\n",
    "        \"avg_METEOR\": sum(meteor_scores) / len(meteor_scores),\n",
    "        \"avg_1-WER\" : sum(wer_scores)    / len(wer_scores),  # Now 1-WER\n",
    "        \"avg_COMET\" : sum(comet_scores)  / len(comet_scores),\n",
    "    })\n",
    "\n",
    "# ---------------------------  write outputs  --------------------------------\n",
    "base, ext = os.path.splitext(csv_file_path)\n",
    "row_metrics_path     = f\"{base}_row_metrics{ext}\"\n",
    "summary_metrics_path = f\"{base}_summary_metrics{ext}\"\n",
    "\n",
    "df.to_csv(row_metrics_path, index=False)\n",
    "pd.DataFrame(summary_rows).to_csv(summary_metrics_path, index=False)\n",
    "\n",
    "print(\"\\n✔ Evaluation finished.\")\n",
    "print(f\"• Row-level metrics saved to  : {row_metrics_path}\")\n",
    "print(f\"• Column-level summary saved to: {summary_metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfacb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grouped plot of the average scores\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert summary rows to DataFrame for plotting\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "# Melt the DataFrame to long format for easier plotting\n",
    "summary_long = summary_df.melt(id_vars='hypothesis_column', \n",
    "                                value_vars=['avg_BLEU', 'avg_METEOR', 'avg_1-WER', 'avg_COMET'],\n",
    "                                var_name='Metric', value_name='Score')\n",
    "# Set the style for seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# No need to convert WER scores anymore since they're already 1-WER\n",
    "# Rename the metrics for clarity\n",
    "summary_long['Metric'] = summary_long['Metric'].replace({\n",
    "    'avg_BLEU': 'BLEU',\n",
    "    'avg_METEOR': 'METEOR',\n",
    "    'avg_1-WER': '1-WER',\n",
    "    'avg_COMET': 'COMET'\n",
    "})\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=summary_long, x='hypothesis_column', y='Score', hue='Metric')\n",
    "plt.title('Average Scores by Models')\n",
    "plt.xlabel('Hypothesis Column')\n",
    "plt.ylabel('Average Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Metric')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=summary_long, x='Metric', y='Score', hue='hypothesis_column')\n",
    "plt.title('Average Scores by Metric')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Average Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Models')\n",
    "plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
