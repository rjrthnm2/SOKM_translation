{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robin\\anaconda3\\envs\\whisperx\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import whisperx\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32= False\n",
    "torch.backends.cudnn.allow_tf32= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu118\n",
      "CUDA version used to build PyTorch: 11.8\n",
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version used to build PyTorch:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract Audio from Video\n",
    "def extract_audio_from_video(video_file: str, audio_file: str):\n",
    "    print(f\"Starting audio extraction from video: {video_file}\")\n",
    "    subprocess.run([\n",
    "        \"ffmpeg\",\n",
    "        \"-i\", video_file,\n",
    "        \"-vn\",  # no video, only audio\n",
    "        \"-acodec\", \"pcm_s16le\",  # audio codec (WAV format)\n",
    "        \"-ar\", \"16000\",  # sample rate\n",
    "        \"-ac\", \"1\",  # number of audio channels\n",
    "        audio_file\n",
    "    ], check=True)\n",
    "    print(f\"Audio extraction completed. Audio saved to: {audio_file}\")\n",
    "\n",
    "# Step 2: Transcribe Audio with WhisperX\n",
    "def transcribe_audio_with_whisperx(audio_file: str):\n",
    "    print(f\"Starting transcription of audio file: {audio_file}\")\n",
    "    device = \"cuda\"\n",
    "    batch_size = 6\n",
    "    compute_type = \"float16\"\n",
    "    model = whisperx.load_model(\"large-v3\", device, vad_method=\"silero\", compute_type=compute_type,language='en')\n",
    "    audio = whisperx.load_audio(audio_file)\n",
    "    transcription_result = model.transcribe(audio, batch_size=batch_size)\n",
    "    print(f\"Transcription complete. Number of segments: {len(transcription_result['segments'])}\")\n",
    "    model_a, metadata = whisperx.load_align_model(language_code=transcription_result[\"language\"], device=device)\n",
    "    aligned_result = whisperx.align(transcription_result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "    print(\"Alignment complete.\")\n",
    "    diarize_model = whisperx.DiarizationPipeline(model_name=\"pyannote/speaker-diarization-3.1\",use_auth_token=\"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\", device=device)\n",
    "    # add min/max number of speakers if known\n",
    "    diarize_segments = diarize_model(audio)\n",
    "    # diarize_model(audio, num_speakers= total_speakers, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "    diarize_result = whisperx.assign_word_speakers(diarize_segments, aligned_result)\n",
    "    # print(result[\"segments\"]) # segments are now assigned speaker IDs\n",
    "    print(\"Diarization complete.\")\n",
    "    del audio\n",
    "    gc.collect()\n",
    "    if 'torch' in globals():\n",
    "        torch.cuda.empty_cache()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if 'torch' in globals():\n",
    "        torch.cuda.empty_cache()\n",
    "    return diarize_result\n",
    "\n",
    "# Step 3: Translate Transcription to Spanish\n",
    "def translate_transcription_to_spanish(transcription_result):\n",
    "    print(\"Starting translation of transcription to Spanish.\")\n",
    "    translator = Translator()\n",
    "    english_texts = [segment[\"text\"] for segment in transcription_result[\"segments\"]]\n",
    "    spanish_translations = []\n",
    "\n",
    "    for idx, text in enumerate(english_texts):\n",
    "        print(f\"Translating segment {idx + 1}/{len(english_texts)}\")\n",
    "        translated = translator.translate(text, src='en', dest='es')\n",
    "        spanish_translations.append(translated.text)\n",
    "    \n",
    "    print(\"Translation to Spanish complete.\")\n",
    "    return spanish_translations\n",
    "\n",
    "# Step 4: Generate SRT File\n",
    "def generate_srt_file(transcription_result, translations, srt_file_path: str):\n",
    "    print(f\"Generating SRT file: {srt_file_path}\")\n",
    "    with open(srt_file_path, \"w\", encoding=\"utf-8\") as srt_file:\n",
    "        for idx, (segment, text) in enumerate(zip(transcription_result[\"segments\"], translations)):\n",
    "            start_time = segment[\"start\"]\n",
    "            end_time = segment[\"end\"]\n",
    "\n",
    "            start_str = f\"{int(start_time // 3600):02}:{int((start_time % 3600) // 60):02}:{int(start_time % 60):02},{int((start_time % 1) * 1000):03}\"\n",
    "            end_str = f\"{int(end_time // 3600):02}:{int((end_time % 3600) // 60):02}:{int(end_time % 60):02},{int((end_time % 1) * 1000):03}\"\n",
    "\n",
    "            srt_file.write(f\"{idx + 1}\\n\")\n",
    "            srt_file.write(f\"{start_str} --> {end_str}\\n\")\n",
    "            srt_file.write(f\"{text}\\n\\n\")\n",
    "    print(f\"SRT file generation complete: {srt_file_path}\")\n",
    "\n",
    "    # Check for potential issues in segment timestamps\n",
    "    for idx, segment in enumerate(transcription_result[\"segments\"]):\n",
    "        if segment[\"end\"] <= segment[\"start\"]:\n",
    "            print(f\"Warning: Misaligned timestamps in segment {idx + 1}. Start time: {segment['start']}, End time: {segment['end']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = r\"D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k.mp4\"\n",
    "# video_file = r\"E:\\HoloOrbits\\Videos\\112923_wg1_red\\EXPORT\\112923_wg1_red_sync.mp4\"\n",
    "base_filename, _ = os.path.splitext(video_file)\n",
    "audio_file = f\"{base_filename}_audio.wav\"\n",
    "english_srt_file_path = f\"{base_filename}_transcript_english.srt\"\n",
    "spanish_srt_file_path = f\"{base_filename}_transcript_spanish.srt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Extracting audio from video.\n",
      "Starting audio extraction from video: E:\\HoloOrbits\\Videos\\112923_wg1_red\\EXPORT\\112923_wg1_red_sync.mp4\n",
      "Audio extraction completed. Audio saved to: E:\\HoloOrbits\\Videos\\112923_wg1_red\\EXPORT\\112923_wg1_red_sync_audio.wav\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract audio from video\n",
    "print(\"Step 1: Extracting audio from video.\")\n",
    "extract_audio_from_video(video_file, audio_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Transcribing audio with WhisperX.\n",
      "Starting transcription of audio file: D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio.wav\n",
      ">>Performing voice activity detection using Silero...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\robin/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription complete. Number of segments: 93\n",
      "Alignment complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robin\\anaconda3\\envs\\whisperx\\lib\\inspect.py:869: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "c:\\Users\\robin\\anaconda3\\envs\\whisperx\\lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diarization complete.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Transcribe audio with WhisperX\n",
    "print(\"Step 2: Transcribing audio with WhisperX.\")\n",
    "transcription_result = transcribe_audio_with_whisperx(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPEAKER_00\n"
     ]
    }
   ],
   "source": [
    "print(transcription_result['segments'][0]['words'][0]['speaker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " there is something inside all of us that calls out for more.\n",
      "61\n",
      "684\n"
     ]
    }
   ],
   "source": [
    "print(transcription_result['segments'][0]['text'])\n",
    "print(len(transcription_result['segments'][0]['text']))\n",
    "print(len(transcription_result['segments']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37467\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(len(transcription_result['segments'])):\n",
    "    # if len(transcription_result['segments'][i]['text']) > 62:  # Filter based on length\n",
    "    a += len(transcription_result['segments'][i]['text'])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english.csv\n"
     ]
    }
   ],
   "source": [
    "#Step 2b: storing temporary csv output\n",
    "# Specify the filename for the CSV file\n",
    "csv_filename = audio_file.rsplit('.',1)[0] +'_english.csv'\n",
    "\n",
    "# Open a CSV file to write to\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['Segment Start', 'Segment End', 'Segment Text','Speaker']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for segment in transcription_result[\"segments\"]:\n",
    "        segment_start = segment['start']\n",
    "        segment_end = segment['end']\n",
    "        segment_text = segment['text']\n",
    "        # Check if the words list is non-empty and if the first word has a 'speaker' key\n",
    "        if segment.get('words') and 'speaker' in segment['words'][0]:\n",
    "                segment_speaker = segment['words'][0]['speaker']\n",
    "        else:\n",
    "                speaker_counts = {}  # Dictionary to count occurrences of each speaker\n",
    "        \n",
    "                for word in segment.get('words', []):  # Ensure 'words' exists\n",
    "                        speaker = word.get('speaker')\n",
    "                        if speaker:  # Only count non-empty speaker values\n",
    "                                speaker_counts[speaker] = speaker_counts.get(speaker, 0) + 1\n",
    "                        \n",
    "                # Determine the most frequent speaker\n",
    "                if speaker_counts:\n",
    "                        segment_speaker = max(speaker_counts, key=speaker_counts.get)\n",
    "                else:\n",
    "                        segment_speaker = 'Unknown'  # Default if no speakers exist\n",
    "        writer.writerow({\n",
    "                'Segment Start': segment_start,\n",
    "                'Segment End': segment_end,\n",
    "                'Segment Text': segment_text,\n",
    "                'Speaker': segment_speaker\n",
    "        })\n",
    "\n",
    "print(f\"Data successfully written to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Generating English SRT file.\n",
      "Generating SRT file: D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_transcript_english.srt\n",
      "SRT file generation complete: D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_transcript_english.srt\n"
     ]
    }
   ],
   "source": [
    " # Step 3: Generate English SRT file\n",
    "print(\"Step 3: Generating English SRT file.\")\n",
    "english_translations = [segment[\"text\"] for segment in transcription_result[\"segments\"]]\n",
    "generate_srt_file(transcription_result, english_translations, english_srt_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create .srt using whisperx utils\n",
    "from whisperx.SubtitlesProcessor import SubtitlesProcessor\n",
    "\n",
    "srt_result = transcription_result\n",
    "srt_result[\"language\"] = 'en'\n",
    "# Remove speaker info from the transcription result\n",
    "for segment in srt_result[\"segments\"]:\n",
    "    segment.pop(\"speaker\", None)\n",
    "    for word in segment.get(\"words\", []):\n",
    "        word.pop(\"speaker\", None)\n",
    "\n",
    "srt_output = f\"{base_filename}_subprocessor.srt\"\n",
    "subtitles_proccessor = SubtitlesProcessor(\n",
    "    srt_result[\"segments\"],\n",
    "    'en', # str, two letter code to identify the language\n",
    "    max_line_length=21, # int, around 100 has been working for me\n",
    "    min_char_length_splitter=21, # int, around 70 has been working for me\n",
    "    is_vtt=False, # bool, true for vtt, false for srt format\n",
    ")\n",
    "subtitles_proccessor.save(srt_output, advanced_splitting=True) # output_path is a str with your desired filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Translate transcription to Spanish\n",
    "print(\"Step 4: Translating transcription to Spanish.\")\n",
    "spanish_translations = translate_transcription_to_spanish(transcription_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Generate Spanish SRT file\n",
    "print(\"Step 5: Generating Spanish SRT file.\")\n",
    "generate_srt_file(transcription_result, spanish_translations, spanish_srt_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate using deepl\n",
    "Cost $7 (monthly cost) + ~$1 (50,000characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepl:Request to DeepL API method=POST url=https://api-free.deepl.com/v2/translate\n",
      "INFO:deepl:DeepL API response status_code=200 url=https://api-free.deepl.com/v2/translate\n",
      "INFO:deepl:Request to DeepL API method=GET url=https://api-free.deepl.com/v2/usage\n",
      "INFO:deepl:DeepL API response status_code=200 url=https://api-free.deepl.com/v2/usage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hay algo dentro de todos nosotros que pide más.\n",
      "Character usage: 37467/500000\n"
     ]
    }
   ],
   "source": [
    "#testing deepl\n",
    "import deepl \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "DEEPL_AUTH_KEY = os.getenv(\"DEEPL_AUTH_KEY\")\n",
    "\n",
    "translator = deepl.Translator(DEEPL_AUTH_KEY)\n",
    "\n",
    "text = transcription_result['segments'][0]['text']\n",
    "translated = translator.translate_text(text, target_lang=\"ES\")\n",
    "\n",
    "print(translated.text)\n",
    "usage = translator.get_usage()\n",
    "print(f\"Character usage: {usage.character.count}/{usage.character.limit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(r'D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_test.csv')\n",
    "df = pd.read_csv(r'D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepl:Request to DeepL API method=GET url=https://api-free.deepl.com/v2/usage\n",
      "INFO:deepl:DeepL API response status_code=200 url=https://api-free.deepl.com/v2/usage\n",
      "INFO:deepl:Request to DeepL API method=POST url=https://api-free.deepl.com/v2/translate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Character usage: 0/500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepl:DeepL API response status_code=200 url=https://api-free.deepl.com/v2/translate\n",
      "INFO:deepl:Request to DeepL API method=GET url=https://api-free.deepl.com/v2/usage\n",
      "INFO:deepl:DeepL API response status_code=200 url=https://api-free.deepl.com/v2/usage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed and saved to '01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_deepl.csv'\n",
      "Final Character usage: 37467/500000\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import deepl \n",
    "load_dotenv()\n",
    "\n",
    "DEEPL_AUTH_KEY = os.getenv(\"DEEPL_AUTH_KEY\")\n",
    "\n",
    "translator = deepl.Translator(DEEPL_AUTH_KEY)\n",
    "\n",
    "usage = translator.get_usage()\n",
    "print(f\"Initial Character usage: {usage.character.count}/{usage.character.limit}\")\n",
    "\n",
    "if 'Segment Text' in df.columns:\n",
    "    # Extract the 'text' column as a list\n",
    "    texts = df['Segment Text'].tolist()\n",
    "\n",
    "    # Translate the texts in batches\n",
    "    df['Translated Text'] = translator.translate_text(texts, target_lang=\"ES\")\n",
    "\n",
    "    # Drop the original 'text' column\n",
    "    df = df.drop(columns=['Segment Text'])\n",
    "\n",
    "    df.to_csv(r'D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_deepl.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(\"Translation completed and saved to '01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_deepl.csv'\")\n",
    "\n",
    "else:\n",
    "    print(\"The 'text' column is not present in the CSV file.\")\n",
    "\n",
    "usage = translator.get_usage()\n",
    "print(f\"Final Character usage: {usage.character.count}/{usage.character.limit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating SRT file: D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_deepl.srt\n",
      "SRT file generation complete: D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_deepl.srt\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "csv_file_path = r'D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_deepl.csv'\n",
    "srt_file_path = r'D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_deepl.srt'\n",
    "\n",
    "# Load the CSV file\n",
    "df2 = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Check if required columns exist\n",
    "required_columns = [\"Segment Start\", \"Segment End\", 'Translated Text']\n",
    "if not all(col in df2.columns for col in required_columns):\n",
    "    raise ValueError(f\"CSV file must contain the following columns: {required_columns}\")\n",
    "\n",
    "# Generate the SRT file\n",
    "print(f\"Generating SRT file: {srt_file_path}\")\n",
    "with open(srt_file_path, \"w\", encoding=\"utf-8\") as srt_file:\n",
    "    for idx in range(len(df2)):\n",
    "        start_time = df2[\"Segment Start\"].iloc[idx]\n",
    "        end_time = df2[\"Segment End\"].iloc[idx]\n",
    "\n",
    "        # Format start and end times\n",
    "        start_str = f\"{int(start_time // 3600):02}:{int((start_time % 3600) // 60):02}:{int(start_time % 60):02},{int((start_time % 1) * 1000):03}\"\n",
    "        end_str = f\"{int(end_time // 3600):02}:{int((end_time % 3600) // 60):02}:{int(end_time % 60):02},{int((end_time % 1) * 1000):03}\"\n",
    "\n",
    "        # Write to SRT file\n",
    "        srt_file.write(f\"{idx + 1}\\n\")\n",
    "        srt_file.write(f\"{start_str} --> {end_str}\\n\")\n",
    "        srt_file.write(f\"{df2['Translated Text'].iloc[idx]}\\n\\n\")\n",
    "\n",
    "print(f\"SRT file generation complete: {srt_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate using Local Helsinki model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(r'D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_test.csv')\n",
    "df = pd.read_csv(r'D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robin\\anaconda3\\envs\\whisperx\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\robin\\anaconda3\\envs\\whisperx\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-tc-big-en-es'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    " \n",
    "def translate_batch(texts, batch_size=8):\n",
    "    translations = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\", unit=\"batch\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        # Generate the translated text\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_length=512, num_beams=8, early_stopping=True)\n",
    "\n",
    "        # Decode the output and store the translated texts\n",
    "        batch_translations = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        translations.extend(batch_translations)\n",
    "    return translations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 81/81 [13:53<00:00, 10.28s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed and saved to '01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_helsinki.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if 'Segment Text' in df.columns:\n",
    "    # Extract the 'text' column as a list\n",
    "    texts = df['Segment Text'].tolist()\n",
    "\n",
    "    # Translate the texts in batches\n",
    "    df['Translated Text'] = translate_batch(texts, batch_size=8)\n",
    "\n",
    "    # Drop the original 'text' column\n",
    "    df = df.drop(columns=['Segment Text'])\n",
    "\n",
    "    df.to_csv(r'D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_helsinki.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(\"Translation completed and saved to '01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_helsinki.csv'\")\n",
    "else:\n",
    "    print(\"The 'text' column is not present in the CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating SRT file: D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_helsinki.srt\n",
      "SRT file generation complete: D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_helsinki.srt\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "csv_file_path = r'D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_helsinki.csv'\n",
    "srt_file_path = r'D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_helsinki.srt'\n",
    "\n",
    "# Load the CSV file\n",
    "df2 = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Check if required columns exist\n",
    "required_columns = [\"Segment Start\", \"Segment End\", 'Translated Text']\n",
    "if not all(col in df2.columns for col in required_columns):\n",
    "    raise ValueError(f\"CSV file must contain the following columns: {required_columns}\")\n",
    "\n",
    "# Generate the SRT file\n",
    "print(f\"Generating SRT file: {srt_file_path}\")\n",
    "with open(srt_file_path, \"w\", encoding=\"utf-8\") as srt_file:\n",
    "    for idx in range(len(df2)):\n",
    "        start_time = df2[\"Segment Start\"].iloc[idx]\n",
    "        end_time = df2[\"Segment End\"].iloc[idx]\n",
    "\n",
    "        # Format start and end times\n",
    "        start_str = f\"{int(start_time // 3600):02}:{int((start_time % 3600) // 60):02}:{int(start_time % 60):02},{int((start_time % 1) * 1000):03}\"\n",
    "        end_str = f\"{int(end_time // 3600):02}:{int((end_time % 3600) // 60):02}:{int(end_time % 60):02},{int((end_time % 1) * 1000):03}\"\n",
    "\n",
    "        # Write to SRT file\n",
    "        srt_file.write(f\"{idx + 1}\\n\")\n",
    "        srt_file.write(f\"{start_str} --> {end_str}\\n\")\n",
    "        srt_file.write(f\"{df2['Translated Text'].iloc[idx]}\\n\\n\")\n",
    "\n",
    "print(f\"SRT file generation complete: {srt_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate using ChatGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay algo dentro de todos nosotros que clama por más.\n"
     ]
    }
   ],
   "source": [
    "#Testing ChatGPT API\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API key\n",
    "client = OpenAI(\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Define the chat messages\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert translator. Translate the following text from English to Spanish.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"There is something inside all of us that calls out for more.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature = 1,\n",
    "    max_tokens = 100,\n",
    ")\n",
    "\n",
    "# Extract and print the translation\n",
    "translation = chat_completion.choices[0].message.content\n",
    "print(f\"{translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/13...\n",
      "Processing batch 2/13...\n",
      "Processing batch 3/13...\n",
      "Processing batch 4/13...\n",
      "Processing batch 5/13...\n",
      "Processing batch 6/13...\n",
      "Processing batch 7/13...\n",
      "Processing batch 8/13...\n",
      "Processing batch 9/13...\n",
      "Processing batch 10/13...\n",
      "Processing batch 11/13...\n",
      "Processing batch 12/13...\n",
      "Processing batch 13/13...\n",
      "Translation completed and saved to 'D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_gpt4o.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API key\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Function to process text in batches\n",
    "def translate_batch(batch, model=\"chatgpt-4o-latest\", temperature=1, max_tokens=500):\n",
    "# def translate_batch(batch, model=\"gpt-4o-mini\", temperature=1, max_tokens=500): # model = \"chatgpt-4o-latest\"\n",
    "    translated_batch = []\n",
    "    for text in batch:\n",
    "        try:\n",
    "            # Call OpenAI API for translation\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are an expert translator. Translate the following text from English to Spanish.\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": text,\n",
    "                    }\n",
    "                ],\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            translation = chat_completion.choices[0].message.content\n",
    "            translated_batch.append(translation)\n",
    "        except Exception as e:\n",
    "            print(f\"Error translating text: {text}. Error: {e}\")\n",
    "            translated_batch.append(None)  # Add None for failed translations\n",
    "    return translated_batch\n",
    "\n",
    "# Main processing function\n",
    "def process_csv(input_file, output_file, batch_size=50):\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    if 'Segment Text' in df.columns:\n",
    "        texts = df['Segment Text'].tolist()\n",
    "        translated_texts = []\n",
    "\n",
    "        # Process texts in batches\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            print(f\"Processing batch {i // batch_size + 1}/{(len(texts) + batch_size - 1) // batch_size}...\")\n",
    "            translated_batch = translate_batch(batch)\n",
    "            translated_texts.extend(translated_batch)\n",
    "\n",
    "        # Add translations to the DataFrame\n",
    "        df['Translated Text'] = translated_texts\n",
    "\n",
    "        # Drop the original 'Segment Text' column\n",
    "        df = df.drop(columns=['Segment Text'])\n",
    "\n",
    "        # Save the updated DataFrame to a new CSV file\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Translation completed and saved to '{output_file}'\")\n",
    "    else:\n",
    "        print(\"The 'Segment Text' column is not present in the CSV file.\")\n",
    "\n",
    "# Define file paths\n",
    "input_csv = r'D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed.csv'\n",
    "output_csv = r'D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_gpt4o.csv'\n",
    "\n",
    "# Run the process\n",
    "process_csv(input_csv, output_csv, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating SRT file: D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed.srt\n",
      "SRT file generation complete: D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed.srt\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "csv_file_path = r'D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_gpt4o.csv'\n",
    "srt_file_path = r'D:\\SOKM\\01 Introduction SoKM 2024 - 2025 4k_audio_english_fixed_translated_gpt4o.srt'\n",
    "\n",
    "# Load the CSV file\n",
    "df2 = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Check if required columns exist\n",
    "required_columns = [\"Segment Start\", \"Segment End\", 'Translated Text']\n",
    "if not all(col in df2.columns for col in required_columns):\n",
    "    raise ValueError(f\"CSV file must contain the following columns: {required_columns}\")\n",
    "\n",
    "# Generate the SRT file\n",
    "print(f\"Generating SRT file: {srt_file_path}\")\n",
    "with open(srt_file_path, \"w\", encoding=\"utf-8\") as srt_file:\n",
    "    for idx in range(len(df2)):\n",
    "        start_time = df2[\"Segment Start\"].iloc[idx]\n",
    "        end_time = df2[\"Segment End\"].iloc[idx]\n",
    "\n",
    "        # Format start and end times\n",
    "        start_str = f\"{int(start_time // 3600):02}:{int((start_time % 3600) // 60):02}:{int(start_time % 60):02},{int((start_time % 1) * 1000):03}\"\n",
    "        end_str = f\"{int(end_time // 3600):02}:{int((end_time % 3600) // 60):02}:{int(end_time % 60):02},{int((end_time % 1) * 1000):03}\"\n",
    "\n",
    "        # Write to SRT file\n",
    "        srt_file.write(f\"{idx + 1}\\n\")\n",
    "        srt_file.write(f\"{start_str} --> {end_str}\\n\")\n",
    "        srt_file.write(f\"{df2['Translated Text'].iloc[idx]}\\n\\n\")\n",
    "\n",
    "print(f\"SRT file generation complete: {srt_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription complete. Number of segments: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_voxpopuli_base_10k_asr_es.pt\" to C:\\Users\\robin/.cache\\torch\\hub\\checkpoints\\wav2vec2_voxpopuli_base_10k_asr_es.pt\n",
      "100%|██████████| 360M/360M [00:03<00:00, 106MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment complete.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting translation of audio file: {audio_file}\")\n",
    "device = \"cuda\"\n",
    "batch_size = 6\n",
    "compute_type = \"float16\"\n",
    "model = whisperx.load_model(\"large-v3\", device, vad_method=\"silero\", compute_type=compute_type,language='es')\n",
    "audio = whisperx.load_audio(audio_file)\n",
    "translation_result = model.transcribe(audio, batch_size=batch_size, task=\"translate\", language=\"es\")\n",
    "print(f\"Transcription complete. Number of segments: {len(translation_result['segments'])}\")\n",
    "model_a, metadata = whisperx.load_align_model(language_code=translation_result[\"language\"], device=device)\n",
    "aligned_result = whisperx.align(translation_result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "print(\"Alignment complete.\")\n",
    "\n",
    "del audio\n",
    "gc.collect()\n",
    "if 'torch' in globals():\n",
    "    torch.cuda.empty_cache()\n",
    "del model\n",
    "gc.collect()\n",
    "if 'torch' in globals():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda\"\n",
    "batch_size = 6\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "result = pipe(audio_file, generate_kwargs={\"language\": \"spanish\"})\n",
    "print(result[\"text\"])\n",
    "print(result[\"chunks\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
